# -*- coding: utf-8 -*-
"""Project 1 supervised learning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iLbynewXqsKa9NCqSxNIYLQqD_L3JxBK

##**Import data**
"""

import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
import seaborn as sns
#!pip install -U scikit-learn==1.4
from pandas.plotting import scatter_matrix


#import dataset
pizza = pd.read_csv("https://raw.githubusercontent.com/550tealeaves/DATA71200_sum2024_550/main/project%201/pizza_v2.csv")
print(pizza.head())

pizza_df=pd.DataFrame(pizza)

"""## **Inspect data**"""



#get information on the data type for each column
pizza.info()

#obtain summary statistics on numeric data
pizza.describe()

"""## Global check of dataset for any missing values"""

pizza.isnull().values.any()

"""# **Splitting dataset**
Predicting the company name based on certain features - had to leave the company feature out of the columns variable because it would split it and then when setting y & X, you can only call one variable.
"""

# import pandas as pd
# import numpy as np
# import os
# import matplotlib.pyplot as plt
# import seaborn as sns


columns = ['topping', 'variant', 'size', 'extra_sauce', 'extra_cheese','extra_mushrooms']
pizza_df = pd.concat([pizza]+[pd.get_dummies(pizza[i],drop_first=True) for i in columns],axis=1)
pizza_df.drop(columns,axis=1,inplace=True)
pizza_df.head(3)

#predicting the company name based on certain features
y = pizza_df['company']
print(y)

#dropped the features that are probably not relevant to the model
X = pizza_df.drop(['company'], axis=1)
print(X)

from sklearn.model_selection import train_test_split

# split data and labels into a training and a test set
#if we don't stratify, then it will randomly take classes
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, stratify=y)

"""# K-Nearest Neighbor"""

#import KNN clasifier and fit to training data
from sklearn.neighbors import KNeighborsClassifier



knn = KNeighborsClassifier(n_neighbors=1)
knn.fit(X_train, y_train)

print("Test set score: {:.2f}".format(knn.score(X_test, y_test)))

# generate list of predictions for y2_test
y_pred = knn.predict(X_test)

# generate a confusion matrix on pizza data - shows where model might be making mistakes
# model failing in the confusion b/w class 2 & class 3
from sklearn.metrics import confusion_matrix

confusion_matrix(y_test, y_pred)

# calculate class-wise precision score on iris data
from sklearn.metrics import precision_score
precision_score(y_test, y_pred, average=None)

# calculate class-wise recall score on iris data
from sklearn.metrics import recall_score
recall_score(y_test, y_pred, average=None)

# calculate overall and class-wise F1-score on iris data
from sklearn.metrics import f1_score

# Calculate metrics globally by counting the total true positives, false negatives and false positives.
print("F1 score of micro")
print(f1_score(y_test, y_pred, average='micro'))
# Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.
print("F1 score of macro")
print(f1_score(y_test, y_pred, average='macro'))
# Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters ‘macro’ to account for label imbalance; it can result in an F-score that is not between precision and recall.
print("F1 score of weighted")
print(f1_score(y_test, y_pred, average='weighted'))

# Class-wise, no averaging
print('F1 score per class')
print(f1_score(y_test, y_pred, average=None))

# calculate AUC score (ROC implementation in scikit-learn only works for binary classification) on iris data
from sklearn.metrics import roc_auc_score
roc_auc_score(y_test, knn.predict_proba(X_test), multi_class='ovr')

from sklearn.linear_model import LogisticRegression
logreg = LogisticRegression().fit(X_train, y_train)

#best performing k (on training set)
knn_gscv.best_params_

#accuracy of best performing k

# best k (8) performs at 0.49)
knn_gscv.best_score_

"""## Cross Validation Results"""

#cross validation results - 8 & 9th k are tied for best and 23rd k is worst performing
knn_gscv.cv_results_

"""## Best Performing k"""

# accuracy of k = 8 on testing data
knn13 = KNeighborsClassifier(n_neighbors=13)
knn13.fit(X_train, y_train)
print("knn score: {}".format(knn13.score(X_test, y_test)))

"""## Worst Performing k"""

# accuracy of k = 23 on testing data
knn2 = KNeighborsClassifier(n_neighbors=2)
knn2.fit(X_train, y_train)
print("knn score: {}".format(knn2.score(X_test, y_test)))

"""# *Why does the worst performing k perform better than the best performing k?!*

# Logistic Regression
"""

from sklearn.linear_model import LogisticRegression
logreg = LogisticRegression().fit(X_train, y_train)

#try increasing max_iter
logreg = LogisticRegression(max_iter=10000).fit(X_train, y_train)

"""## Import Grid Search"""

from sklearn.model_selection import GridSearchCV


#dictionary of all values of c
param_grid = {"C": [0.001,0.1, 1, 10]}

#grid search on all values of k in dictionary for logist regression
logreg_gscv = GridSearchCV(logreg, param_grid, cv=5)
logreg_gscv.fit(X_train, y_train)

#best performing c (on training set)
logreg_gscv.best_params_

#accuracy of best performing c
logreg_gscv.best_score_

"""## See that the model has a stronger prediction accuracy on logistic regression instead of k nearest neighbors"""

#cross validation results
logreg_gscv.cv_results_

"""# Import Linear SVG & do Grid Search"""

from sklearn.svm import LinearSVC
linearsvc = LinearSVC().fit(X_train, y_train)

#dictionary of all values of c
param_grid = {"C": [0.001,0.1, 1, 10]}

#grid search on all values of c in dictionary for linear svc
linearsvc_gscv = GridSearchCV(linearsvc, param_grid, cv=5)
linearsvc_gscv.fit(X_train, y_train)

#try increasing max_iter for linear svc and rerun grid search
linearsvc = LinearSVC(max_iter=10000).fit(X_train, y_train)

linearsvc_gscv = GridSearchCV(linearsvc, param_grid, cv=5)
linearsvc_gscv.fit(X_train, y_train)

#best performing c (on training set)
logreg_gscv.best_params_

#accuracy of best performing c
logreg_gscv.best_score_

#cross validation results
logreg_gscv.cv_results_

"""## Best Performing"""

# accuracy of C = 1 on testing data
logreg1 = LogisticRegression(C=1,max_iter=10000).fit(X_train, y_train)
logreg1.fit(X_train, y_train)
print("logreg score: {}".format(logreg1.score(X_test, y_test)))

# accuracy of C = 1 on testing data
linearsvc1 = LogisticRegression(C=1,max_iter=1000).fit(X_train, y_train)
linearsvc1.fit(X_train, y_train)
print("logreg score: {}".format(linearsvc1.score(X_test, y_test)))

# precision, recall, f1-score, plus support (number of instances of class)
from sklearn.metrics import classification_report
print(classification_report(y_test, linearsvc1.predict(X_test)))

"""## Worst Performing"""

# accuracy of C = 0.01 on testing data
logreg001 = LogisticRegression(C=0.01,max_iter=10000).fit(X_train, y_train)
logreg001.fit(X_train, y_train)
print("logreg score: {}".format(logreg001.score(X_test, y_test)))

# accuracy of C = 0.01 on testing data
linearsvc001 = LogisticRegression(C=0.01,max_iter=10000).fit(X_train, y_train)
linearsvc001.fit(X_train, y_train)
print("logreg score: {}".format(linearsvc001.score(X_test, y_test)))

"""# Histogram plot seems to indicate the data is slightly right-skewed."""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt
pizza.hist(bins=30, figsize=(20,15), color='pink', ec='purple') #changed fill and edge color
plt.show()

"""### Scatterplot suggests that overall price and diameter size are more-or-less directly proportional."""

pizza.plot(kind="scatter", x="price_cad", y="diameter_in", color="forestgreen")

pizza.plot(kind="scatter", x="price_cad", y="diameter_in", alpha=0.6, color='purple')

pizza.plot(kind="scatter", x="price_cad", y="diameter_in", alpha=0.9,
         s=pizza["price_cad"]/100, label="price_cad",
         c="price_cad", cmap=plt.get_cmap("jet"), colorbar=True,
     )
plt.legend()

#Can see effects of the data skewed
from pandas.plotting import scatter_matrix

attributes = ["price_cad", "diameter_in"] #will plot these 4 col of data
scatter_matrix(pizza[attributes], figsize=(12, 8), color='red', ec='purple')

"""### Correlation matrix shows a strong positive correlation between diameter inches and prices in Canadian dollars"""

corr_matrix = pizza_df[attributes].corr()
corr_matrix['price_cad'].sort_values(ascending=False)

from sklearn.preprocessing import OrdinalEncoder # just to raise an ImportError if Scikit-Learn < 0.20
from sklearn.preprocessing import OneHotEncoder

pizza_cat = pizza['price_cad'].values.reshape(-1,1)



cat_encoder = OneHotEncoder()
pizza_cat_1hot = cat_encoder.fit_transform(pizza_cat)
pizza_cat_1hot

pizza_cat_1hot.toarray()

"""# **Binning**"""

from sklearn.preprocessing import KBinsDiscretizer
kb = KBinsDiscretizer(n_bins=10, strategy='uniform')
kb.fit(pizza['price_cad'].values.reshape(-1, 1))
print("bin edges: \n", kb.bin_edges_)

binned = kb.transform(pizza['price_cad'].values.reshape(-1, 1))
binned

#Create plot of bins
plt.plot(pizza['price_cad'].values.reshape(-1, 1), color='violet')
plt.show()

plt.plot(binned.argmax(1), color='violet')
plt.show()

#Create histogram of price with 10 bins
plt.hist(pizza['price_cad'].values.reshape(-1, 1), bins=10, color='green')
plt.show()

#Create histogram of diameter with 10 bins
plt.hist(pizza['diameter_in'].values.reshape(-1, 1), bins=10, color='purple')
plt.show()

"""# **Transformations**
Will apply the below transformations for price_cad & diameter_in


1.   Squaring
2.   Cubing
3.   np.log
4.   np.exp

ORIGINAL - price_cad
"""

plt.hist(pizza['price_cad'].values.reshape(-1, 1))
plt.show()

"""SQUARED - price_cad"""

plt.hist(pizza['price_cad'].values.reshape(-1, 1)**2, bins=10)
plt.show()

"""CUBED - price_cad"""

plt.hist(pizza['price_cad'].values.reshape(-1, 1)**3, bins=10)
plt.show()

"""NP.LOG - price_cad

"""

plt.hist(np.log(pizza['price_cad'].values.reshape(-1, 1)), bins=10)
plt.show()

"""NP.EXP - price_cad"""

plt.hist(np.exp(pizza['price_cad'].values.reshape(-1, 1)), bins=10)
plt.show()

"""## Transformation for diameter_in - rerun the same ones as above

ORIGINAL - diameter_in
"""

plt.hist(pizza['diameter_in'].values.reshape(-1, 1))
plt.show()

"""SQUARED - diameter_in"""

plt.hist(pizza['diameter_in'].values.reshape(-1, 1)**2, bins=10)
plt.show()

"""CUBED - diameter_in"""

plt.hist(pizza['diameter_in'].values.reshape(-1, 1)**3, bins=10)
plt.show()

"""NP.LOG - diameter_in"""

plt.hist(np.log(pizza['diameter_in'].values.reshape(-1, 1)), bins=10)
plt.show()

"""NP.EXP - diameter_in"""

plt.hist(np.exp(pizza['diameter_in'].values.reshape(-1, 1)), bins=10)
plt.show()

"""## Transformation scatter matrices"""

#Original scatter matrix for price_cad and diameter_in

attributes = ["price_cad", "diameter_in"]
scatter_matrix(pizza[attributes], figsize=(12, 8))

"""Squared scattered matrices"""

scatter_matrix(pizza[attributes]**2, figsize=(12, 8))

"""CUBED scattered matrices"""

scatter_matrix(pizza[attributes]**3, figsize=(12, 8))